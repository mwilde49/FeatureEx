{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline for Medical Image Analysis\n",
    "\n",
    "This notebook performs inference on new images using:\n",
    "1. CNN features (ResNet18)\n",
    "2. Radiomics features\n",
    "3. Fusion features (CNN + Radiomics)\n",
    "\n",
    "Results are saved to `inference/jobs.xlsx` with two sheets:\n",
    "- `predictions`: Image predictions from each model\n",
    "- `feature_scores`: Extracted feature vectors for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Import Libraries\n",
    "# ========================\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('radiomics').setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Setup Directories and Paths\n",
    "# ========================\n",
    "BASE_DIR = Path(r'C:/FeatureEx')\n",
    "INFERENCE_DIR = BASE_DIR / 'inference'\n",
    "DONE_DIR = INFERENCE_DIR / 'done'\n",
    "MASKS_DIR = BASE_DIR / 'masks'\n",
    "JOBS_FILE = INFERENCE_DIR / 'jobs.xlsx'\n",
    "MODEL_PATH = BASE_DIR / 'best_resnet_model.pth'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "INFERENCE_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Inference directory: {INFERENCE_DIR}\")\n",
    "print(f\"Done directory: {DONE_DIR}\")\n",
    "print(f\"Jobs file: {JOBS_FILE}\")\n",
    "print(f\"\\nDirectories ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Initialize Excel File\n",
    "# ========================\n",
    "def initialize_jobs_file():\n",
    "    \"\"\"Create jobs.xlsx with two sheets if it doesn't exist\"\"\"\n",
    "    if not JOBS_FILE.exists():\n",
    "        # Create predictions sheet\n",
    "        predictions_df = pd.DataFrame(columns=[\n",
    "            'filename', 'timestamp', 'cnn_prediction', \n",
    "            'radiomic_prediction', 'fusion_prediction'\n",
    "        ])\n",
    "        \n",
    "        # Create feature_scores sheet (will be dynamically sized)\n",
    "        feature_scores_df = pd.DataFrame(columns=['filename'])\n",
    "        \n",
    "        # Write to Excel\n",
    "        with pd.ExcelWriter(JOBS_FILE, engine='openpyxl') as writer:\n",
    "            predictions_df.to_excel(writer, sheet_name='predictions', index=False)\n",
    "            feature_scores_df.to_excel(writer, sheet_name='feature_scores', index=False)\n",
    "        \n",
    "        print(f\"Created new jobs file: {JOBS_FILE}\")\n",
    "    else:\n",
    "        print(f\"Jobs file already exists: {JOBS_FILE}\")\n",
    "\n",
    "initialize_jobs_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Define ResNet Model\n",
    "# ========================\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-based model for classification and feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3, feature_dim=512, pretrained=True):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first conv layer for grayscale (1 channel) input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Remove the original fully connected layer\n",
    "        num_features = self.resnet.fc.in_features  # 512 for ResNet18\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Custom feature extraction and classification layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_features, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet backbone\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        # Extract custom features\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "print(\"ResNet model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Load Trained Models\n",
    "# ========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = ResNetFeatureExtractor(num_classes=3, feature_dim=512, pretrained=False)\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    cnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded CNN model from {MODEL_PATH}\")\n",
    "    print(f\"  - Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "else:\n",
    "    print(f\"WARNING: Model file not found at {MODEL_PATH}\")\n",
    "    print(\"Please train the model first using Test_FE_PCA.ipynb\")\n",
    "\n",
    "cnn_model = cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "\n",
    "print(\"\\nCNN model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================",
    "# Load Trained Classifiers",
    "# ========================",
    "import pickle",
    "",
    "MODELS_DIR = BASE_DIR / 'models'",
    "",
    "print(\"Loading trained classifiers...\")",
    "print(\"=\"*70)",
    "",
    "# Load metadata",
    "metadata_path = MODELS_DIR / 'metadata.pkl'",
    "if metadata_path.exists():",
    "    with open(metadata_path, 'rb') as f:",
    "        metadata = pickle.load(f)",
    "    print(f\"Metadata loaded:\")",
    "    print(f\"  - Fusion type: {metadata.get('fusion_type', 'unknown')}\")",
    "    print(f\"  - CNN features: {metadata.get('cnn_features_dim', 'N/A')}\")",
    "    print(f\"  - Radiomic features: {metadata.get('radio_features_dim', 'N/A')}\")",
    "    fusion_type = metadata.get('fusion_type', 'none')",
    "else:",
    "    print(\"\u26a0 Warning: metadata.pkl not found\")",
    "    fusion_type = 'none'",
    "",
    "print()",
    "",
    "# Load CNN classifier (for reference, though we use the ResNet model directly)",
    "cnn_classifier_path = MODELS_DIR / 'clf_cnn.pkl'",
    "if cnn_classifier_path.exists():",
    "    with open(cnn_classifier_path, 'rb') as f:",
    "        cnn_classifier = pickle.load(f)",
    "    print(\"\u2713 CNN classifier loaded\")",
    "else:",
    "    cnn_classifier = None",
    "    print(\"\u26a0 CNN classifier not found (will use ResNet model predictions)\")",
    "",
    "# Load radiomics classifier",
    "radiomics_classifier_path = MODELS_DIR / 'clf_radio.pkl'",
    "if radiomics_classifier_path.exists():",
    "    with open(radiomics_classifier_path, 'rb') as f:",
    "        radiomics_classifier = pickle.load(f)",
    "    print(\"\u2713 Radiomics classifier loaded\")",
    "else:",
    "    radiomics_classifier = None",
    "    print(\"\u26a0 Radiomics classifier not found\")",
    "",
    "# Load fusion classifier",
    "fusion_classifier_path = MODELS_DIR / 'clf_fusion.pkl'",
    "if fusion_classifier_path.exists():",
    "    with open(fusion_classifier_path, 'rb') as f:",
    "        fusion_classifier = pickle.load(f)",
    "    print(\"\u2713 Fusion classifier loaded\")",
    "else:",
    "    fusion_classifier = None",
    "    print(\"\u26a0 Fusion classifier not found\")",
    "",
    "# Load preprocessing objects if needed for PCA fusion",
    "scaler_cnn = None",
    "scaler_radio = None",
    "pca_cnn = None",
    "pca_radio = None",
    "",
    "if fusion_type == 'pca':",
    "    print(\"\\nLoading PCA preprocessing objects...\")",
    "",
    "    scaler_cnn_path = MODELS_DIR / 'scaler_cnn.pkl'",
    "    if scaler_cnn_path.exists():",
    "        with open(scaler_cnn_path, 'rb') as f:",
    "            scaler_cnn = pickle.load(f)",
    "        print(\"  \u2713 CNN scaler loaded\")",
    "",
    "    scaler_radio_path = MODELS_DIR / 'scaler_radio.pkl'",
    "    if scaler_radio_path.exists():",
    "        with open(scaler_radio_path, 'rb') as f:",
    "            scaler_radio = pickle.load(f)",
    "        print(\"  \u2713 Radiomics scaler loaded\")",
    "",
    "    pca_cnn_path = MODELS_DIR / 'pca_cnn.pkl'",
    "    if pca_cnn_path.exists():",
    "        with open(pca_cnn_path, 'rb') as f:",
    "            pca_cnn = pickle.load(f)",
    "        print(\"  \u2713 CNN PCA loaded\")",
    "",
    "    pca_radio_path = MODELS_DIR / 'pca_radio.pkl'",
    "    if pca_radio_path.exists():",
    "        with open(pca_radio_path, 'rb') as f:",
    "            pca_radio = pickle.load(f)",
    "        print(\"  \u2713 Radiomics PCA loaded\")",
    "",
    "# Initialize radiomics extractor",
    "radiomics_extractor = featureextractor.RadiomicsFeatureExtractor()",
    "",
    "print()",
    "print(\"=\"*70)",
    "print(\"Summary:\")",
    "print(f\"  CNN classifier: {'Available' if cnn_classifier else 'Using ResNet model'}\")",
    "print(f\"  Radiomics classifier: {'Available' if radiomics_classifier else 'Not available'}\")",
    "print(f\"  Fusion classifier: {'Available' if fusion_classifier else 'Not available'}\")",
    "print(f\"  Fusion type: {fusion_type}\")",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Image Preprocessing Functions\n",
    "# ========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess image for CNN inference\"\"\"\n",
    "    pil_image = Image.open(image_path).convert('L')\n",
    "    image_tensor = transform(pil_image).unsqueeze(0)\n",
    "    return image_tensor, pil_image\n",
    "\n",
    "def create_mask_for_image(pil_image):\n",
    "    \"\"\"Create binary mask for radiomics extraction\"\"\"\n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(pil_image.resize((224, 224))).astype(np.float32)\n",
    "    \n",
    "    # Create mask with background border\n",
    "    mask_array = np.ones(img_array.shape, dtype=np.uint8)\n",
    "    mask_array[0, :] = 0\n",
    "    mask_array[-1, :] = 0\n",
    "    mask_array[:, 0] = 0\n",
    "    mask_array[:, -1] = 0\n",
    "    \n",
    "    # Convert to SimpleITK\n",
    "    image_sitk = sitk.GetImageFromArray(img_array)\n",
    "    mask_sitk = sitk.GetImageFromArray(mask_array)\n",
    "    mask_sitk = sitk.Cast(mask_sitk, sitk.sitkUInt8)\n",
    "    mask_sitk.CopyInformation(image_sitk)\n",
    "    \n",
    "    return image_sitk, mask_sitk\n",
    "\n",
    "print(\"Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================",
    "# Feature Extraction Functions",
    "# ========================",
    "def extract_cnn_features(image_tensor):",
    "    \"\"\"Extract CNN features and prediction\"\"\"",
    "    with torch.no_grad():",
    "        image_tensor = image_tensor.to(device)",
    "        logits, features = cnn_model(image_tensor)",
    "        prediction = torch.argmax(logits, dim=1).item() + 1  # +1 to match labels 1-3",
    "        features_np = features.cpu().numpy().flatten()",
    "    return prediction, features_np",
    "",
    "def extract_radiomics_features(pil_image):",
    "    \"\"\"Extract radiomics features and prediction\"\"\"",
    "    try:",
    "        # Create mask",
    "        image_sitk, mask_sitk = create_mask_for_image(pil_image)",
    "",
    "        # Extract features",
    "        result = radiomics_extractor.execute(image_sitk, mask_sitk)",
    "",
    "        # Filter to only original features",
    "        features = {k: v for k, v in result.items() if k.startswith(\"original\")}",
    "        feature_vector = np.array([float(v) for v in features.values()])",
    "",
    "        # Predict using radiomics classifier if available",
    "        if radiomics_classifier is not None:",
    "            # Labels are 0-2 internally, need to convert to 1-3",
    "            prediction = radiomics_classifier.predict([feature_vector])[0] + 1",
    "        else:",
    "            prediction = None  # No classifier available",
    "",
    "        return prediction, feature_vector",
    "    except Exception as e:",
    "        print(f\"Error extracting radiomics features: {e}\")",
    "        return None, None",
    "",
    "def extract_fusion_features(cnn_features, radiomic_features):",
    "    \"\"\"Combine CNN and radiomics features for fusion prediction\"\"\"",
    "    if cnn_features is None or radiomic_features is None:",
    "        return None, None",
    "",
    "    try:",
    "        # Check if we need to apply PCA preprocessing",
    "        if fusion_type == 'pca' and all([scaler_cnn, scaler_radio, pca_cnn, pca_radio]):",
    "            # Apply PCA preprocessing",
    "            cnn_scaled = scaler_cnn.transform([cnn_features])",
    "            radio_scaled = scaler_radio.transform([radiomic_features])",
    "",
    "            cnn_pca = pca_cnn.transform(cnn_scaled)",
    "            radio_pca = pca_radio.transform(radio_scaled)",
    "",
    "            # Concatenate PCA features",
    "            fusion_features = np.concatenate([cnn_pca.flatten(), radio_pca.flatten()])",
    "        else:",
    "            # Simple concatenation (no PCA)",
    "            fusion_features = np.concatenate([cnn_features, radiomic_features])",
    "",
    "        # Predict using fusion classifier if available",
    "        if fusion_classifier is not None:",
    "            # Labels are 0-2 internally, need to convert to 1-3",
    "            prediction = fusion_classifier.predict([fusion_features])[0] + 1",
    "        else:",
    "            prediction = None  # No classifier available",
    "",
    "        return prediction, fusion_features",
    "",
    "    except Exception as e:",
    "        print(f\"Error in fusion prediction: {e}\")",
    "        return None, None",
    "",
    "print(\"Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Main Inference Function\n",
    "# ========================\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Process a single image through the inference pipeline\n",
    "    Returns: dict with predictions and features\n",
    "    \"\"\"\n",
    "    filename = Path(image_path).name\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image_tensor, pil_image = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Extract CNN features and prediction\n",
    "    print(\"  - Extracting CNN features...\")\n",
    "    cnn_pred, cnn_features = extract_cnn_features(image_tensor)\n",
    "    print(f\"    CNN prediction: {cnn_pred}\")\n",
    "    \n",
    "    # Extract radiomics features and prediction\n",
    "    print(\"  - Extracting radiomics features...\")\n",
    "    radiomic_pred, radiomic_features = extract_radiomics_features(pil_image)\n",
    "    if radiomic_pred is not None:\n",
    "        print(f\"    Radiomics prediction: {radiomic_pred}\")\n",
    "    else:\n",
    "        print(\"    Radiomics prediction: N/A (no classifier)\")\n",
    "    \n",
    "    # Extract fusion features and prediction\n",
    "    print(\"  - Extracting fusion features...\")\n",
    "    fusion_pred, fusion_features = extract_fusion_features(cnn_features, radiomic_features)\n",
    "    if fusion_pred is not None:\n",
    "        print(f\"    Fusion prediction: {fusion_pred}\")\n",
    "    else:\n",
    "        print(\"    Fusion prediction: N/A (no classifier)\")\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'filename': filename,\n",
    "        'timestamp': timestamp,\n",
    "        'predictions': {\n",
    "            'cnn': cnn_pred,\n",
    "            'radiomic': radiomic_pred,\n",
    "            'fusion': fusion_pred\n",
    "        },\n",
    "        'features': {\n",
    "            'cnn': cnn_features,\n",
    "            'radiomic': radiomic_features,\n",
    "            'fusion': fusion_features\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Main inference function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Save Results to Excel\n",
    "# ========================\n",
    "def save_results_to_excel(results):\n",
    "    \"\"\"\n",
    "    Append results to jobs.xlsx\n",
    "    \"\"\"\n",
    "    # Read existing data\n",
    "    predictions_df = pd.read_excel(JOBS_FILE, sheet_name='predictions')\n",
    "    feature_scores_df = pd.read_excel(JOBS_FILE, sheet_name='feature_scores')\n",
    "    \n",
    "    # Add prediction row\n",
    "    new_prediction = {\n",
    "        'filename': results['filename'],\n",
    "        'timestamp': results['timestamp'],\n",
    "        'cnn_prediction': results['predictions']['cnn'],\n",
    "        'radiomic_prediction': results['predictions']['radiomic'],\n",
    "        'fusion_prediction': results['predictions']['fusion']\n",
    "    }\n",
    "    predictions_df = pd.concat([predictions_df, pd.DataFrame([new_prediction])], ignore_index=True)\n",
    "    \n",
    "    # Add feature scores row\n",
    "    feature_row = {'filename': results['filename']}\n",
    "    \n",
    "    # Add CNN features\n",
    "    if results['features']['cnn'] is not None:\n",
    "        for i, val in enumerate(results['features']['cnn']):\n",
    "            feature_row[f'cnn_feature_{i+1}'] = val\n",
    "    \n",
    "    # Add radiomic features\n",
    "    if results['features']['radiomic'] is not None:\n",
    "        for i, val in enumerate(results['features']['radiomic']):\n",
    "            feature_row[f'radiomic_feature_{i+1}'] = val\n",
    "    \n",
    "    # Add fusion features\n",
    "    if results['features']['fusion'] is not None:\n",
    "        for i, val in enumerate(results['features']['fusion']):\n",
    "            feature_row[f'fusion_feature_{i+1}'] = val\n",
    "    \n",
    "    feature_scores_df = pd.concat([feature_scores_df, pd.DataFrame([feature_row])], ignore_index=True)\n",
    "    \n",
    "    # Write back to Excel\n",
    "    with pd.ExcelWriter(JOBS_FILE, engine='openpyxl', mode='w') as writer:\n",
    "        predictions_df.to_excel(writer, sheet_name='predictions', index=False)\n",
    "        feature_scores_df.to_excel(writer, sheet_name='feature_scores', index=False)\n",
    "    \n",
    "    print(f\"  \u2713 Results saved to {JOBS_FILE}\")\n",
    "\n",
    "print(\"Excel save function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Move Processed Image\n",
    "# ========================\n",
    "def move_to_done(image_path):\n",
    "    \"\"\"\n",
    "    Move processed image to 'done' subdirectory\n",
    "    \"\"\"\n",
    "    filename = Path(image_path).name\n",
    "    destination = DONE_DIR / filename\n",
    "    \n",
    "    # Handle duplicate filenames\n",
    "    if destination.exists():\n",
    "        base = destination.stem\n",
    "        ext = destination.suffix\n",
    "        counter = 1\n",
    "        while destination.exists():\n",
    "            destination = DONE_DIR / f\"{base}_{counter}{ext}\"\n",
    "            counter += 1\n",
    "    \n",
    "    shutil.move(str(image_path), str(destination))\n",
    "    print(f\"  \u2713 Moved to {destination}\")\n",
    "\n",
    "print(\"Move function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Run Inference Pipeline\n",
    "# ========================\n",
    "def run_inference_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline: Process all images in inference directory\n",
    "    \"\"\"\n",
    "    # Find all image files in inference directory\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(INFERENCE_DIR.glob(f'*{ext}')))\n",
    "        image_files.extend(list(INFERENCE_DIR.glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    # Filter out files in subdirectories\n",
    "    image_files = [f for f in image_files if f.parent == INFERENCE_DIR]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found in inference directory.\")\n",
    "        print(f\"Please place images in: {INFERENCE_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Found {len(image_files)} image(s) to process\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Processing {image_path.name}...\")\n",
    "            \n",
    "            # Run inference\n",
    "            results = process_image(image_path)\n",
    "            \n",
    "            # Save to Excel\n",
    "            save_results_to_excel(results)\n",
    "            \n",
    "            # Move to done\n",
    "            move_to_done(image_path)\n",
    "            \n",
    "            print(f\"  \u2713 Completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 ERROR processing {image_path.name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Inference pipeline complete!\")\n",
    "    print(f\"Results saved to: {JOBS_FILE}\")\n",
    "    print(f\"Processed images moved to: {DONE_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"Pipeline function defined!\")\n",
    "print(\"\\nReady to run inference!\")\n",
    "print(\"Execute the next cell to process all images in the inference directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# EXECUTE INFERENCE\n",
    "# ========================\n",
    "# Run this cell to process all images\n",
    "\n",
    "run_inference_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# View Results\n",
    "# ========================\n",
    "# View the predictions sheet\n",
    "print(\"Predictions:\")\n",
    "predictions_df = pd.read_excel(JOBS_FILE, sheet_name='predictions')\n",
    "display(predictions_df)\n",
    "\n",
    "print(\"\\nFeature Scores (first 5 columns):\")\n",
    "feature_scores_df = pd.read_excel(JOBS_FILE, sheet_name='feature_scores')\n",
    "display(feature_scores_df.iloc[:, :5])  # Show first 5 columns only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FeatureEx)",
   "language": "python",
   "name": "featureex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}