{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline for Medical Image Analysis\n",
    "\n",
    "This notebook performs inference on new images using:\n",
    "1. CNN features (ResNet18)\n",
    "2. Radiomics features\n",
    "3. Fusion features (CNN + Radiomics)\n",
    "\n",
    "Results are saved to `inference/jobs.xlsx` with two sheets:\n",
    "- `predictions`: Image predictions from each model\n",
    "- `feature_scores`: Extracted feature vectors for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Import Libraries\n",
    "# ========================\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('radiomics').setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Setup Directories and Paths\n",
    "# ========================\n",
    "BASE_DIR = Path(r'C:/FeatureEx')\n",
    "INFERENCE_DIR = BASE_DIR / 'inference'\n",
    "DONE_DIR = INFERENCE_DIR / 'done'\n",
    "MASKS_DIR = BASE_DIR / 'masks'\n",
    "JOBS_FILE = INFERENCE_DIR / 'jobs.xlsx'\n",
    "MODEL_PATH = BASE_DIR / 'best_resnet_model.pth'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "INFERENCE_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Inference directory: {INFERENCE_DIR}\")\n",
    "print(f\"Done directory: {DONE_DIR}\")\n",
    "print(f\"Jobs file: {JOBS_FILE}\")\n",
    "print(f\"\\nDirectories ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Initialize Excel File\n",
    "# ========================\n",
    "def initialize_jobs_file():\n",
    "    \"\"\"Create jobs.xlsx with two sheets if it doesn't exist\"\"\"\n",
    "    if not JOBS_FILE.exists():\n",
    "        # Create predictions sheet\n",
    "        predictions_df = pd.DataFrame(columns=[\n",
    "            'filename', 'timestamp', 'cnn_prediction', \n",
    "            'radiomic_prediction', 'fusion_prediction'\n",
    "        ])\n",
    "        \n",
    "        # Create feature_scores sheet (will be dynamically sized)\n",
    "        feature_scores_df = pd.DataFrame(columns=['filename'])\n",
    "        \n",
    "        # Write to Excel\n",
    "        with pd.ExcelWriter(JOBS_FILE, engine='openpyxl') as writer:\n",
    "            predictions_df.to_excel(writer, sheet_name='predictions', index=False)\n",
    "            feature_scores_df.to_excel(writer, sheet_name='feature_scores', index=False)\n",
    "        \n",
    "        print(f\"Created new jobs file: {JOBS_FILE}\")\n",
    "    else:\n",
    "        print(f\"Jobs file already exists: {JOBS_FILE}\")\n",
    "\n",
    "initialize_jobs_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Define ResNet Model\n",
    "# ========================\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-based model for classification and feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3, feature_dim=512, pretrained=True):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify first conv layer for grayscale (1 channel) input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Remove the original fully connected layer\n",
    "        num_features = self.resnet.fc.in_features  # 512 for ResNet18\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Custom feature extraction and classification layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_features, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet backbone\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        # Extract custom features\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "print(\"ResNet model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Load Trained Models\n",
    "# ========================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = ResNetFeatureExtractor(num_classes=3, feature_dim=512, pretrained=False)\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    cnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded CNN model from {MODEL_PATH}\")\n",
    "    print(f\"  - Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "else:\n",
    "    print(f\"WARNING: Model file not found at {MODEL_PATH}\")\n",
    "    print(\"Please train the model first using Test_FE_PCA.ipynb\")\n",
    "\n",
    "cnn_model = cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "\n",
    "print(\"\\nCNN model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Load Trained Classifiers\n",
    "# ========================\n",
    "# Note: You'll need to train and save these classifiers from Test_FE_PCA.ipynb\n",
    "# For now, we'll use the CNN model's predictions directly\n",
    "\n",
    "# Placeholder for radiomics classifier\n",
    "radiomics_classifier = None  # Load from pickle if saved\n",
    "\n",
    "# Placeholder for fusion classifier  \n",
    "fusion_classifier = None  # Load from pickle if saved\n",
    "\n",
    "# Initialize radiomics extractor\n",
    "radiomics_extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "\n",
    "print(\"Classifiers initialized!\")\n",
    "print(\"NOTE: If radiomics/fusion classifiers are not trained yet,\")\n",
    "print(\"      only CNN predictions will be available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Image Preprocessing Functions\n",
    "# ========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess image for CNN inference\"\"\"\n",
    "    pil_image = Image.open(image_path).convert('L')\n",
    "    image_tensor = transform(pil_image).unsqueeze(0)\n",
    "    return image_tensor, pil_image\n",
    "\n",
    "def create_mask_for_image(pil_image):\n",
    "    \"\"\"Create binary mask for radiomics extraction\"\"\"\n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(pil_image.resize((224, 224))).astype(np.float32)\n",
    "    \n",
    "    # Create mask with background border\n",
    "    mask_array = np.ones(img_array.shape, dtype=np.uint8)\n",
    "    mask_array[0, :] = 0\n",
    "    mask_array[-1, :] = 0\n",
    "    mask_array[:, 0] = 0\n",
    "    mask_array[:, -1] = 0\n",
    "    \n",
    "    # Convert to SimpleITK\n",
    "    image_sitk = sitk.GetImageFromArray(img_array)\n",
    "    mask_sitk = sitk.GetImageFromArray(mask_array)\n",
    "    mask_sitk = sitk.Cast(mask_sitk, sitk.sitkUInt8)\n",
    "    mask_sitk.CopyInformation(image_sitk)\n",
    "    \n",
    "    return image_sitk, mask_sitk\n",
    "\n",
    "print(\"Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Feature Extraction Functions\n",
    "# ========================\n",
    "def extract_cnn_features(image_tensor):\n",
    "    \"\"\"Extract CNN features and prediction\"\"\"\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        logits, features = cnn_model(image_tensor)\n",
    "        prediction = torch.argmax(logits, dim=1).item() + 1  # +1 to match labels 1-3\n",
    "        features_np = features.cpu().numpy().flatten()\n",
    "    return prediction, features_np\n",
    "\n",
    "def extract_radiomics_features(pil_image):\n",
    "    \"\"\"Extract radiomics features and prediction\"\"\"\n",
    "    try:\n",
    "        # Create mask\n",
    "        image_sitk, mask_sitk = create_mask_for_image(pil_image)\n",
    "        \n",
    "        # Extract features\n",
    "        result = radiomics_extractor.execute(image_sitk, mask_sitk)\n",
    "        \n",
    "        # Filter to only original features\n",
    "        features = {k: v for k, v in result.items() if k.startswith(\"original\")}\n",
    "        feature_vector = np.array([float(v) for v in features.values()])\n",
    "        \n",
    "        # Predict using radiomics classifier if available\n",
    "        if radiomics_classifier is not None:\n",
    "            prediction = radiomics_classifier.predict([feature_vector])[0]\n",
    "        else:\n",
    "            prediction = None  # No classifier available\n",
    "        \n",
    "        return prediction, feature_vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting radiomics features: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_fusion_features(cnn_features, radiomic_features):\n",
    "    \"\"\"Combine CNN and radiomics features for fusion prediction\"\"\"\n",
    "    if cnn_features is None or radiomic_features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Concatenate features\n",
    "    fusion_features = np.concatenate([cnn_features, radiomic_features])\n",
    "    \n",
    "    # Predict using fusion classifier if available\n",
    "    if fusion_classifier is not None:\n",
    "        prediction = fusion_classifier.predict([fusion_features])[0]\n",
    "    else:\n",
    "        prediction = None  # No classifier available\n",
    "    \n",
    "    return prediction, fusion_features\n",
    "\n",
    "print(\"Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Main Inference Function\n",
    "# ========================\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Process a single image through the inference pipeline\n",
    "    Returns: dict with predictions and features\n",
    "    \"\"\"\n",
    "    filename = Path(image_path).name\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image_tensor, pil_image = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # Extract CNN features and prediction\n",
    "    print(\"  - Extracting CNN features...\")\n",
    "    cnn_pred, cnn_features = extract_cnn_features(image_tensor)\n",
    "    print(f\"    CNN prediction: {cnn_pred}\")\n",
    "    \n",
    "    # Extract radiomics features and prediction\n",
    "    print(\"  - Extracting radiomics features...\")\n",
    "    radiomic_pred, radiomic_features = extract_radiomics_features(pil_image)\n",
    "    if radiomic_pred is not None:\n",
    "        print(f\"    Radiomics prediction: {radiomic_pred}\")\n",
    "    else:\n",
    "        print(\"    Radiomics prediction: N/A (no classifier)\")\n",
    "    \n",
    "    # Extract fusion features and prediction\n",
    "    print(\"  - Extracting fusion features...\")\n",
    "    fusion_pred, fusion_features = extract_fusion_features(cnn_features, radiomic_features)\n",
    "    if fusion_pred is not None:\n",
    "        print(f\"    Fusion prediction: {fusion_pred}\")\n",
    "    else:\n",
    "        print(\"    Fusion prediction: N/A (no classifier)\")\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'filename': filename,\n",
    "        'timestamp': timestamp,\n",
    "        'predictions': {\n",
    "            'cnn': cnn_pred,\n",
    "            'radiomic': radiomic_pred,\n",
    "            'fusion': fusion_pred\n",
    "        },\n",
    "        'features': {\n",
    "            'cnn': cnn_features,\n",
    "            'radiomic': radiomic_features,\n",
    "            'fusion': fusion_features\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Main inference function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Save Results to Excel\n",
    "# ========================\n",
    "def save_results_to_excel(results):\n",
    "    \"\"\"\n",
    "    Append results to jobs.xlsx\n",
    "    \"\"\"\n",
    "    # Read existing data\n",
    "    predictions_df = pd.read_excel(JOBS_FILE, sheet_name='predictions')\n",
    "    feature_scores_df = pd.read_excel(JOBS_FILE, sheet_name='feature_scores')\n",
    "    \n",
    "    # Add prediction row\n",
    "    new_prediction = {\n",
    "        'filename': results['filename'],\n",
    "        'timestamp': results['timestamp'],\n",
    "        'cnn_prediction': results['predictions']['cnn'],\n",
    "        'radiomic_prediction': results['predictions']['radiomic'],\n",
    "        'fusion_prediction': results['predictions']['fusion']\n",
    "    }\n",
    "    predictions_df = pd.concat([predictions_df, pd.DataFrame([new_prediction])], ignore_index=True)\n",
    "    \n",
    "    # Add feature scores row\n",
    "    feature_row = {'filename': results['filename']}\n",
    "    \n",
    "    # Add CNN features\n",
    "    if results['features']['cnn'] is not None:\n",
    "        for i, val in enumerate(results['features']['cnn']):\n",
    "            feature_row[f'cnn_feature_{i+1}'] = val\n",
    "    \n",
    "    # Add radiomic features\n",
    "    if results['features']['radiomic'] is not None:\n",
    "        for i, val in enumerate(results['features']['radiomic']):\n",
    "            feature_row[f'radiomic_feature_{i+1}'] = val\n",
    "    \n",
    "    # Add fusion features\n",
    "    if results['features']['fusion'] is not None:\n",
    "        for i, val in enumerate(results['features']['fusion']):\n",
    "            feature_row[f'fusion_feature_{i+1}'] = val\n",
    "    \n",
    "    feature_scores_df = pd.concat([feature_scores_df, pd.DataFrame([feature_row])], ignore_index=True)\n",
    "    \n",
    "    # Write back to Excel\n",
    "    with pd.ExcelWriter(JOBS_FILE, engine='openpyxl', mode='w') as writer:\n",
    "        predictions_df.to_excel(writer, sheet_name='predictions', index=False)\n",
    "        feature_scores_df.to_excel(writer, sheet_name='feature_scores', index=False)\n",
    "    \n",
    "    print(f\"  ✓ Results saved to {JOBS_FILE}\")\n",
    "\n",
    "print(\"Excel save function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Move Processed Image\n",
    "# ========================\n",
    "def move_to_done(image_path):\n",
    "    \"\"\"\n",
    "    Move processed image to 'done' subdirectory\n",
    "    \"\"\"\n",
    "    filename = Path(image_path).name\n",
    "    destination = DONE_DIR / filename\n",
    "    \n",
    "    # Handle duplicate filenames\n",
    "    if destination.exists():\n",
    "        base = destination.stem\n",
    "        ext = destination.suffix\n",
    "        counter = 1\n",
    "        while destination.exists():\n",
    "            destination = DONE_DIR / f\"{base}_{counter}{ext}\"\n",
    "            counter += 1\n",
    "    \n",
    "    shutil.move(str(image_path), str(destination))\n",
    "    print(f\"  ✓ Moved to {destination}\")\n",
    "\n",
    "print(\"Move function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Run Inference Pipeline\n",
    "# ========================\n",
    "def run_inference_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline: Process all images in inference directory\n",
    "    \"\"\"\n",
    "    # Find all image files in inference directory\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(INFERENCE_DIR.glob(f'*{ext}')))\n",
    "        image_files.extend(list(INFERENCE_DIR.glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    # Filter out files in subdirectories\n",
    "    image_files = [f for f in image_files if f.parent == INFERENCE_DIR]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found in inference directory.\")\n",
    "        print(f\"Please place images in: {INFERENCE_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Found {len(image_files)} image(s) to process\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Processing {image_path.name}...\")\n",
    "            \n",
    "            # Run inference\n",
    "            results = process_image(image_path)\n",
    "            \n",
    "            # Save to Excel\n",
    "            save_results_to_excel(results)\n",
    "            \n",
    "            # Move to done\n",
    "            move_to_done(image_path)\n",
    "            \n",
    "            print(f\"  ✓ Completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ ERROR processing {image_path.name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Inference pipeline complete!\")\n",
    "    print(f\"Results saved to: {JOBS_FILE}\")\n",
    "    print(f\"Processed images moved to: {DONE_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"Pipeline function defined!\")\n",
    "print(\"\\nReady to run inference!\")\n",
    "print(\"Execute the next cell to process all images in the inference directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# EXECUTE INFERENCE\n",
    "# ========================\n",
    "# Run this cell to process all images\n",
    "\n",
    "run_inference_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# View Results\n",
    "# ========================\n",
    "# View the predictions sheet\n",
    "print(\"Predictions:\")\n",
    "predictions_df = pd.read_excel(JOBS_FILE, sheet_name='predictions')\n",
    "display(predictions_df)\n",
    "\n",
    "print(\"\\nFeature Scores (first 5 columns):\")\n",
    "feature_scores_df = pd.read_excel(JOBS_FILE, sheet_name='feature_scores')\n",
    "display(feature_scores_df.iloc[:, :5])  # Show first 5 columns only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FeatureEx)",
   "language": "python",
   "name": "featureex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
