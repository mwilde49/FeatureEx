{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 3D Radiomic Feature Extraction\n## Extract radiomics features from 3D medical imaging volumes\n\nThis notebook extracts radiomic features from 3D medical images:\n- **Input:** Raw 3D NIfTI volumes from imagesTr/labelsTr\n- **Features:** Shape, Texture (GLCM, GLRLM, GLSZM), Intensity-based radiomic features\n- **Output:** Feature matrices saved as pickle and CSV\n- **Attention Mask:** All non-background labels treated as single region of interest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import radiomics\n",
    "from radiomics import featureextractor\n",
    "import logging\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('radiomics').setLevel(logging.ERROR)\n",
    "\n",
    "print(f\"PyRadiomics version: {radiomics.__version__}\")\n",
    "print(f\"Radiomics successfully imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nBASE_DIR = Path('C:/FeatureEx')\nIMAGES_DIR = BASE_DIR / 'imagesTr' / 'imagesTr'\nLABELS_DIR = BASE_DIR / 'labelsTr' / 'labelsTr'\nOUTPUT_DIR = BASE_DIR / 'radiomics_3d'\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Settings\nSTRUCTURE_CLASSES = [1, 2, 3, 4]  # Classes 1-4 are structures (0 is background)\n\nprint(f\"Input directories:\")\nprint(f\"  Images: {IMAGES_DIR}\")\nprint(f\"  Labels: {LABELS_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"\\nExtraction settings:\")\nprint(f\"  Structure classes: {STRUCTURE_CLASSES}\")\nprint(f\"  Attention mask: All non-background labels combined into single ROI\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Radiomics Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom parameter file for 3D radiomics\n",
    "params = {\n",
    "    'binWidth': 25,  # 25 HU bin width for CT-like data\n",
    "    'resampledPixelSpacing': None,  # Keep original spacing\n",
    "    'interpolator': 'sitkBSpline',\n",
    "    'label': 1,  # Will be updated per extraction\n",
    "    'imageType': {\n",
    "        'Original': {},\n",
    "        'Wavelet': {'wavelet': 'coif1'},\n",
    "    },\n",
    "    'featureClass': {\n",
    "        'shape': {},\n",
    "        'firstorder': {},\n",
    "        'glcm': {},\n",
    "        'glrlm': {},\n",
    "        'glszm': {},\n",
    "        'ngtdm': {},\n",
    "        'gldm': {},\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Radiomics parameters configured:\")\n",
    "print(f\"  Bin width: {params['binWidth']}\")\n",
    "print(f\"  Image types: {list(params['imageType'].keys())}\")\n",
    "print(f\"  Feature classes: {list(params['featureClass'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get File Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matching image-label pairs\n",
    "image_files = sorted([f for f in IMAGES_DIR.glob('*.nii*')])\n",
    "label_files = sorted([f for f in LABELS_DIR.glob('*.nii*')])\n",
    "\n",
    "# Create mapping\n",
    "image_map = {f.stem: f for f in image_files}\n",
    "label_map = {f.stem: f for f in label_files}\n",
    "\n",
    "# Find matching pairs\n",
    "matching_pairs = set(image_map.keys()) & set(label_map.keys())\n",
    "file_pairs = [(image_map[name], label_map[name]) for name in sorted(matching_pairs)]\n",
    "\n",
    "print(f\"File pairs found: {len(file_pairs)}\")\n",
    "print(f\"\\nFirst 5 pairs:\")\n",
    "for img_path, lbl_path in file_pairs[:5]:\n",
    "    print(f\"  {img_path.name} <-> {lbl_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Multi-class Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_aoi_mask(label_data, structure_classes):\n    \"\"\"\n    Create binary attention mask for all non-background structures.\n    All structures are treated as a single region of interest.\n    \n    Args:\n        label_data: 3D label array\n        structure_classes: List of structure class labels (1-4)\n    \n    Returns:\n        Binary mask array where all non-background labels are 1\n    \"\"\"\n    # Create mask for all non-background classes (structures 1-4)\n    mask = np.isin(label_data, structure_classes).astype(np.uint8)\n    return mask\n\ndef validate_mask(mask_data, min_voxels=100):\n    \"\"\"\n    Validate mask has sufficient voxels.\n    \n    Args:\n        mask_data: Binary mask array\n        min_voxels: Minimum required voxels\n    \n    Returns:\n        Boolean indicating validity\n    \"\"\"\n    voxel_count = np.count_nonzero(mask_data)\n    return voxel_count >= min_voxels\n\nprint(\"Mask helper functions defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Radiomics Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_features(image_path, label_path, extractor, structure_classes):\n    \"\"\"\n    Extract radiomics features for the combined area of interest.\n    \n    All non-background labels (classes 1-4) are treated as a single ROI.\n    \n    Args:\n        image_path: Path to image NIfTI\n        label_path: Path to label NIfTI\n        extractor: RadiomicsFeatureExtractor instance\n        structure_classes: List of structure class labels\n    \n    Returns:\n        Dictionary of features or None if extraction failed\n    \"\"\"\n    try:\n        # Load data\n        image_nib = nib.load(image_path)\n        image_array = image_nib.get_fdata()\n        \n        label_nib = nib.load(label_path)\n        label_array = label_nib.get_fdata()\n        \n        # Create binary mask for all non-background structures\n        mask_array = create_aoi_mask(label_array, structure_classes)\n        \n        # Validate mask\n        if not validate_mask(mask_array):\n            return None\n        \n        # Save mask to temporary NIfTI\n        mask_nib = nib.Nifti1Image(mask_array.astype(np.uint8), label_nib.affine)\n        mask_path = Path('/tmp/temp_mask.nii.gz')\n        nib.save(mask_nib, mask_path)\n        \n        # Extract features\n        features = extractor.execute(str(image_path), str(mask_path))\n        \n        # Clean up temp file\n        mask_path.unlink(missing_ok=True)\n        \n        return dict(features)\n    \n    except Exception as e:\n        return None\n\nprint(\"Feature extraction function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Features from All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize extractor\nextractor = featureextractor.RadiomicsFeatureExtractor()\n\n# Initialize storage\nall_features = []\nfeature_names = None\nsample_ids = []\nextraction_log = []\n\nprint(f\"Starting feature extraction from {len(file_pairs)} image-label pairs...\\n\")\n\n# Extract features from each pair\nfor pair_idx, (img_path, lbl_path) in enumerate(tqdm(file_pairs), 1):\n    sample_name = img_path.stem\n    log_entry = {'sample': sample_name, 'status': 'failed', 'features_extracted': False}\n    \n    # Extract features from combined ROI (all non-background structures)\n    features_dict = extract_features(img_path, lbl_path, extractor, STRUCTURE_CLASSES)\n    \n    if features_dict:\n        # Store features\n        all_features.append(features_dict)\n        sample_ids.append(sample_name)\n        log_entry['status'] = 'success'\n        log_entry['features_extracted'] = True\n        \n        # Extract feature names from first successful extraction\n        if feature_names is None:\n            feature_names = list(features_dict.keys())\n    \n    extraction_log.append(log_entry)\n\nprint(f\"\\nFeature extraction complete!\")\nprint(f\"Total samples with extracted features: {len(all_features)}\")\nprint(f\"Success rate: {len(all_features) / len(file_pairs) * 100:.1f}%\")\nprint(f\"Total feature types: {len(feature_names) if feature_names else 0}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convert to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DataFrame from extracted features\nif all_features:\n    features_df = pd.DataFrame(all_features)\n    features_df['sample_id'] = sample_ids\n    \n    print(f\"Features DataFrame created:\")\n    print(f\"  Samples: {len(features_df)}\")\n    print(f\"  Features: {len(feature_names) if feature_names else 0}\")\n    print(f\"  Total columns: {len(features_df.columns)}\")\n    print(f\"  Shape: {features_df.shape}\")\n    print(f\"\\nFirst 5 feature columns: {list(features_df.columns[:5])}\")\nelse:\n    print(\"No features were successfully extracted!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combine and Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create combined feature matrix (all classes concatenated)\n",
    "combined_features_list = []\n",
    "class_labels_list = []\n",
    "sample_ids_combined = []\n",
    "\n",
    "for class_idx, class_label in enumerate(STRUCTURE_CLASSES):\n",
    "    df_key = f'class_{class_label}'\n",
    "    if df_key in feature_dfs:\n",
    "        df = feature_dfs[df_key]\n",
    "        combined_features_list.append(df.values)\n",
    "        class_labels_list.extend([class_label] * len(df))\n",
    "        sample_ids_combined.extend(df.index.tolist())\n",
    "\n",
    "# Combine and create DataFrame\n",
    "if combined_features_list:\n",
    "    combined_array = np.vstack(combined_features_list)\n",
    "    combined_df = pd.DataFrame(\n",
    "        combined_array,\n",
    "        columns=feature_names if feature_names else [f'feature_{i}' for i in range(combined_array.shape[1])]\n",
    "    )\n",
    "    combined_df['class'] = class_labels_list\n",
    "    \n",
    "    print(f\"Combined feature matrix:\")\n",
    "    print(f\"  Shape: {combined_df.shape}\")\n",
    "    print(f\"  Samples: {len(combined_df)}\")\n",
    "    print(f\"  Features: {len(feature_names) if feature_names else 0}\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for class_label in STRUCTURE_CLASSES:\n",
    "        count = (combined_df['class'] == class_label).sum()\n",
    "        print(f\"  Class {class_label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Statistics and Summary"
   ]
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 9. Feature Statistics and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature statistics\nif feature_names:\n    print(f\"Feature statistics:\")\n    print(f\"\\nTop 10 features by variance:\")\n    \n    feature_variance = features_df[feature_names].var()\n    top_features = feature_variance.nlargest(10)\n    \n    for idx, (feat_name, variance) in enumerate(top_features.items(), 1):\n        print(f\"  {idx}. {feat_name}: {variance:.4f}\")\n    \n    print(f\"\\nFeature summary:\")\n    print(f\"  Total feature types: {len(feature_names)}\")\n    print(f\"  Total radiomic features: {len(feature_names) * len(features_df)}\")\n    print(f\"  Feature classes: {len(set([feat.split('_')[0] for feat in feature_names]))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Features to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save features as CSV\nif all_features:\n    output_csv = OUTPUT_DIR / 'radiomics_3d_features.csv'\n    features_df.to_csv(output_csv, index=False)\n    print(f\"Saved features: {output_csv.name}\")\n    \n    # Also save without sample_id for feature analysis\n    output_features_only = OUTPUT_DIR / 'radiomics_3d_features_only.csv'\n    features_df[feature_names].to_csv(output_features_only, index=False)\n    print(f\"Saved features (no IDs): {output_features_only.name}\")\nelse:\n    print(\"No features to save!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Features as Pickle (for sklearn compatibility)"
   ]
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 11. Save Features as Pickle (for sklearn compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save as pickle for later use with sklearn\npickle_data = {\n    'features_df': features_df,\n    'feature_names': feature_names,\n    'sample_ids': sample_ids,\n    'metadata': {\n        'total_samples': len(file_pairs),\n        'successful_extractions': len(all_features),\n        'success_rate': len(all_features) / len(file_pairs) * 100,\n        'total_features': len(feature_names) if feature_names else 0,\n        'roi_type': 'combined_all_structures',\n        'structure_classes': STRUCTURE_CLASSES,\n        'extraction_log': extraction_log\n    }\n}\n\npickle_path = OUTPUT_DIR / 'radiomics_3d_features.pkl'\nwith open(pickle_path, 'wb') as f:\n    pickle.dump(pickle_data, f)\n\nprint(f\"\\nSaved pickle: {pickle_path.name}\")\nprint(f\"Pickle contains:\")\nprint(f\"  - Features DataFrame\")\nprint(f\"  - Feature names\")\nprint(f\"  - Sample IDs\")\nprint(f\"  - Metadata and extraction log\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Configuration Summary"
   ]
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 12. Create Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config_summary = {\n    'extraction_info': {\n        'total_samples': len(file_pairs),\n        'successful_extractions': len(all_features),\n        'success_rate': f\"{len(all_features) / len(file_pairs) * 100:.1f}%\",\n        'extraction_log_path': str(OUTPUT_DIR / 'extraction_log.json')\n    },\n    'region_of_interest': {\n        'roi_type': 'combined_all_structures',\n        'description': 'All non-background labels (1-4) treated as single attention mask',\n        'structure_classes': STRUCTURE_CLASSES\n    },\n    'features': {\n        'total_feature_types': len(feature_names) if feature_names else 0,\n        'feature_names_sample': feature_names[:10] if feature_names else [],\n        'num_features_in_sample': 10,\n        'total_features_truncated': len(feature_names) - 10 if feature_names and len(feature_names) > 10 else 0\n    },\n    'output_files': {\n        'features_csv': 'radiomics_3d_features.csv',\n        'features_only_csv': 'radiomics_3d_features_only.csv',\n        'pickle': 'radiomics_3d_features.pkl',\n        'config_summary': 'radiomics_3d_config.json',\n        'extraction_log': 'extraction_log.json'\n    }\n}\n\n# Save config\nconfig_path = OUTPUT_DIR / 'radiomics_3d_config.json'\nwith open(config_path, 'w') as f:\n    json.dump(config_summary, f, indent=2)\n\nprint(f\"Configuration summary:\")\nfor key, val in config_summary.items():\n    print(f\"\\n{key}:\")\n    if isinstance(val, dict):\n        for k, v in val.items():\n            print(f\"  {k}: {v}\")\n    else:\n        print(f\"  {val}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Extraction Log"
   ]
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 13. Save Extraction Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save detailed extraction log\nlog_path = OUTPUT_DIR / 'extraction_log.json'\nwith open(log_path, 'w') as f:\n    json.dump(extraction_log, f, indent=2)\n\nprint(f\"Extraction log saved to: {log_path.name}\")\n\n# Print summary\nsuccessful = sum(1 for entry in extraction_log if entry['features_extracted'])\nprint(f\"\\nExtraction summary:\")\nprint(f\"  Total samples processed: {len(extraction_log)}\")\nprint(f\"  Successful extractions: {successful}\")\nprint(f\"  Failed extractions: {len(extraction_log) - successful}\")\nprint(f\"  Success rate: {(successful/len(extraction_log))*100:.1f}%\")"
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 14. Load and Verify Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test loading the pickle file\nwith open(pickle_path, 'rb') as f:\n    loaded_data = pickle.load(f)\n\nprint(f\"Verification - Loaded pickle contents:\")\nprint(f\"  Keys: {list(loaded_data.keys())}\")\nprint(f\"  DataFrame shape: {loaded_data['features_df'].shape}\")\nprint(f\"  Number of features: {len(loaded_data['feature_names'])}\")\nprint(f\"  Number of samples: {len(loaded_data['sample_ids'])}\")\nprint(f\"  ROI type: {loaded_data['metadata']['roi_type']}\")\nprint(f\"  Success rate: {loaded_data['metadata']['success_rate']:.1f}%\")\n\nprint(f\"\\nDataFrame preview:\")\nprint(loaded_data['features_df'].head(3))"
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 15. Feature Usage Example"
   ]
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 15. Feature Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using extracted features for analysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nprint(\"Example: Feature analysis with extracted radiomic features\\n\")\n\nif loaded_data['features_df'] is not None and loaded_data['feature_names']:\n    df = loaded_data['features_df']\n    feature_names_list = loaded_data['feature_names']\n    \n    print(f\"Features extracted from combined ROI (all non-background structures):\")\n    print(f\"  Total samples: {len(df)}\")\n    print(f\"  Total features: {len(feature_names_list)}\")\n    \n    # Basic statistics\n    print(f\"\\nFeature statistics:\")\n    print(f\"  Min values: {df[feature_names_list].min().min():.4f}\")\n    print(f\"  Max values: {df[feature_names_list].max().max():.4f}\")\n    print(f\"  Mean values: {df[feature_names_list].mean().mean():.4f}\")\n    print(f\"  Std values: {df[feature_names_list].std().mean():.4f}\")\n    \n    print(f\"\\nTop 10 features by variance:\")\n    feature_var = df[feature_names_list].var()\n    for i, (feat, var) in enumerate(feature_var.nlargest(10).items(), 1):\n        print(f\"  {i}. {feat}: {var:.6f}\")"
  },
  {
   "cell_type": {
    "text": "markdown"
   },
   "source": [
    "## 16. Summary and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"3D RADIOMIC FEATURE EXTRACTION - COMPLETE\")\nprint(\"=\"*70)\n\nprint(f\"\\nOutput Directory: {OUTPUT_DIR}\")\nprint(f\"\\nROI Configuration:\")\nprint(f\"  Type: Combined all structures (attention mask)\")\nprint(f\"  Description: All non-background labels (1-4) as single region\")\nprint(f\"  Classes: {STRUCTURE_CLASSES}\")\n\nprint(f\"\\nExtraction Results:\")\nprint(f\"  Total samples: {len(file_pairs)}\")\nprint(f\"  Successfully extracted: {len(all_features)}\")\nprint(f\"  Success rate: {len(all_features)/len(file_pairs)*100:.1f}%\")\nprint(f\"  Features per sample: {len(feature_names) if feature_names else 0}\")\n\nprint(f\"\\nGenerated Files:\")\noutput_files = list(OUTPUT_DIR.glob('*'))\nfor file in sorted(output_files):\n    if file.is_file():\n        size_mb = file.stat().st_size / (1024*1024)\n        print(f\"  - {file.name} ({size_mb:.2f} MB)\")\n\nprint(f\"\\nNext Steps:\")\nprint(f\"  1. Load features: pickle.load(open('{pickle_path}', 'rb'))\")\nprint(f\"  2. Access DataFrame: loaded_data['features_df']\")\nprint(f\"  3. Get features: loaded_data['features_df'][loaded_data['feature_names']]\")\nprint(f\"  4. Combine with CNN features from ResNet3D_Classification.ipynb\")\nprint(f\"  5. Perform multi-modal classification and fusion analysis\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}