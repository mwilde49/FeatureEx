{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNN and Radiograph Feature Fusion - PCA and LASSO Classification\n",
    "## Feature Fusion, Dimensionality Reduction, and Classification Analysis\n",
    "\n",
    "This notebook:\n",
    "- Loads CNN features from deep learning models\n",
    "- Loads non-diagnostic radiograph features\n",
    "- Fuses CNN and radiograph features into unified feature set\n",
    "- Performs class-based PCA for dimensionality reduction\n",
    "- Uses LASSO regression for feature selection\n",
    "- Trains and evaluates multi-class classification model\n",
    "- Reports comprehensive ML metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path('C:/FeatureEx')\n",
    "RADIOMICS_DIR = BASE_DIR / 'radiomics_3d'\n",
    "CNN_FEATURES_DIR = BASE_DIR / 'cnn_features'  # Directory containing CNN features\n",
    "RADIOGRAPH_FEATURES_DIR = BASE_DIR / 'radiograph_features'  # Directory containing radiograph features\n",
    "METADATA_FILE = BASE_DIR / 'classification_metadata.xlsx'\n",
    "OUTPUT_DIR = BASE_DIR / 'fusion_pca_results'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "NUM_PCA_COMPONENTS = 50  # Number of PCA components to retain\n",
    "NUM_CLASSES = 5  # Number of classification classes\n",
    "CLASS_NAMES = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "LASSO_MAX_ITER = 5000\n",
    "LASSO_ALPHA_MIN = 0.0001\n",
    "LASSO_ALPHA_MAX = 1.0\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"CNN features directory: {CNN_FEATURES_DIR}\")\n",
    "print(f\"Radiograph features directory: {RADIOGRAPH_FEATURES_DIR}\")\n",
    "print(f\"Metadata file: {METADATA_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  PCA components: {NUM_PCA_COMPONENTS}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  LASSO alpha range: [{LASSO_ALPHA_MIN}, {LASSO_ALPHA_MAX}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN features\n",
    "print(\"Loading CNN features...\")\n",
    "cnn_features_file = CNN_FEATURES_DIR / 'cnn_features.pkl'\n",
    "\n",
    "with open(cnn_features_file, 'rb') as f:\n",
    "    cnn_data = pickle.load(f)\n",
    "\n",
    "# Extract CNN features and metadata\n",
    "cnn_features_df = cnn_data['features_df']\n",
    "cnn_feature_names = cnn_data['feature_names']\n",
    "cnn_sample_ids = cnn_data['sample_ids']\n",
    "\n",
    "print(f\"Loaded CNN features:\")\n",
    "print(f\"  Samples: {len(cnn_features_df)}\")\n",
    "print(f\"  Features: {len(cnn_feature_names)}\")\n",
    "print(f\"  Feature names (first 5): {cnn_feature_names[:5]}\")\n",
    "print(f\"  DataFrame shape: {cnn_features_df.shape}\")\n",
    "print(f\"  DataFrame columns (first 10): {list(cnn_features_df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Radiograph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load radiograph (non-diagnostic) features\n",
    "print(\"Loading radiograph features...\")\n",
    "radiograph_features_file = RADIOGRAPH_FEATURES_DIR / 'radiograph_features.pkl'\n",
    "\n",
    "with open(radiograph_features_file, 'rb') as f:\n",
    "    radiograph_data = pickle.load(f)\n",
    "\n",
    "# Extract radiograph features and metadata\n",
    "radiograph_features_df = radiograph_data['features_df']\n",
    "radiograph_feature_names = radiograph_data['feature_names']\n",
    "radiograph_sample_ids = radiograph_data['sample_ids']\n",
    "\n",
    "print(f\"Loaded radiograph features:\")\n",
    "print(f\"  Samples: {len(radiograph_features_df)}\")\n",
    "print(f\"  Features: {len(radiograph_feature_names)}\")\n",
    "print(f\"  Feature names (first 5): {radiograph_feature_names[:5]}\")\n",
    "print(f\"  DataFrame shape: {radiograph_features_df.shape}\")\n",
    "print(f\"  DataFrame columns (first 10): {list(radiograph_features_df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fuse CNN and Radiograph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse CNN and radiograph features\n",
    "print(\"Fusing CNN and radiograph features...\")\n",
    "\n",
    "# Ensure both dataframes have sample_id column\n",
    "if 'sample_id' not in cnn_features_df.columns:\n",
    "    cnn_features_df['sample_id'] = cnn_sample_ids\n",
    "if 'sample_id' not in radiograph_features_df.columns:\n",
    "    radiograph_features_df['sample_id'] = radiograph_sample_ids\n",
    "\n",
    "# Merge on sample_id to ensure alignment\n",
    "# Use inner join to keep only samples present in both datasets\n",
    "merged_df = cnn_features_df.merge(\n",
    "    radiograph_features_df,\n",
    "    on='sample_id',\n",
    "    how='inner',\n",
    "    suffixes=('_cnn', '_radiograph')\n",
    ")\n",
    "\n",
    "# Combine feature names\n",
    "all_feature_names = list(cnn_feature_names) + list(radiograph_feature_names)\n",
    "\n",
    "print(f\"\\nFeature fusion results:\")\n",
    "print(f\"  CNN features: {len(cnn_feature_names)}\")\n",
    "print(f\"  Radiograph features: {len(radiograph_feature_names)}\")\n",
    "print(f\"  Total fused features: {len(all_feature_names)}\")\n",
    "print(f\"  Samples with both feature types: {len(merged_df)}\")\n",
    "print(f\"  Merged dataframe shape: {merged_df.shape}\")\n",
    "\n",
    "# Verify no duplicate feature names\n",
    "if len(all_feature_names) != len(set(all_feature_names)):\n",
    "    print(\"\\nWARNING: Duplicate feature names detected!\")\n",
    "    duplicates = [name for name in all_feature_names if all_feature_names.count(name) > 1]\n",
    "    print(f\"Duplicates: {set(duplicates)}\")\nelse:\n",
    "    print(\"\\nAll feature names are unique!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Classification Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading classification metadata...\")\n",
    "metadata_df = pd.read_excel(METADATA_FILE, sheet_name='samples')\n",
    "\n",
    "print(f\"Loaded metadata:\")\n",
    "print(f\"  Samples: {len(metadata_df)}\")\n",
    "print(f\"  Columns: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nMetadata preview:\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(metadata_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(metadata_df['split'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge Fused Features with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fused features with labels\n",
    "print(\"Merging fused features with classification labels...\")\n",
    "\n",
    "# Create mapping from sample_id to label\n",
    "label_map = dict(zip(metadata_df['sample_id'], metadata_df['label']))\n",
    "\n",
    "# Add labels to merged dataframe\n",
    "merged_df['class_label'] = merged_df['sample_id'].map(label_map)\n",
    "\n",
    "# Check for unmatched samples\n",
    "unmatched = merged_df['class_label'].isna().sum()\n",
    "if unmatched > 0:\n",
    "    print(f\"WARNING: {unmatched} samples have no matching label\")\n",
    "    print(f\"Removing unmatched samples...\")\n",
    "    merged_df = merged_df.dropna(subset=['class_label'])\n",
    "    merged_df['class_label'] = merged_df['class_label'].astype(int)\n",
    "\n",
    "print(f\"\\nMerged data with labels:\")\n",
    "print(f\"  Total samples with labels: {len(merged_df)}\")\n",
    "print(f\"  Total fused features: {len(all_feature_names)}\")\n",
    "print(f\"\\nClass distribution in merged data:\")\n",
    "print(merged_df['class_label'].value_counts().sort_index())\n",
    "\n",
    "# Prepare data for analysis\n",
    "X = merged_df[all_feature_names].values  # Feature matrix\n",
    "y = merged_df['class_label'].values       # Labels\n",
    "sample_ids_final = merged_df['sample_id'].values\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X (fused features): {X.shape}\")\n",
    "print(f\"  y (labels): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preprocessing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "print(\"Standardizing fused features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Feature scaling completed:\")\n",
    "print(f\"  Mean (should be ~0): {X_scaled.mean(axis=0)[:5]}\")\n",
    "print(f\"  Std (should be ~1): {X_scaled.std(axis=0)[:5]}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "scaler_path = OUTPUT_DIR / 'feature_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\nScaler saved to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Class-Based PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "print(\"Performing PCA on fused features...\")\n",
    "pca = PCA(n_components=NUM_PCA_COMPONENTS)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"PCA completed:\")\n",
    "print(f\"  Components: {pca.n_components_}\")\n",
    "print(f\"  Explained variance ratio: {pca.explained_variance_ratio_[:5]}\")\n",
    "print(f\"  Cumulative variance: {np.cumsum(pca.explained_variance_ratio_)[-1]:.4f}\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, NUM_PCA_COMPONENTS + 1), pca.explained_variance_ratio_)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA Scree Plot (Fused Features)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'pca_variance_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA plot saved to: {OUTPUT_DIR / 'pca_variance_analysis.png'}\")\n",
    "\n",
    "# Save PCA model\n",
    "pca_path = OUTPUT_DIR / 'pca_model.pkl'\n",
    "with open(pca_path, 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "print(f\"PCA model saved to: {pca_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. PCA Visualization by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "print(\"Visualizing PCA results by class...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 2D PCA plot (PC1 vs PC2)\n",
    "for class_label in np.unique(y):\n",
    "    mask = y == class_label\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   label=f'Class {class_label}', alpha=0.7, s=50)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} var)')\n",
    "axes[0].set_title('PCA: PC1 vs PC2 (Fused Features)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D PCA plot (PC1 vs PC3)\n",
    "for class_label in np.unique(y):\n",
    "    mask = y == class_label\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 2], \n",
    "                   label=f'Class {class_label}', alpha=0.7, s=50)\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)')\n",
    "axes[1].set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.2%} var)')\n",
    "axes[1].set_title('PCA: PC1 vs PC3 (Fused Features)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'pca_scatter_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA scatter plot saved to: {OUTPUT_DIR / 'pca_scatter_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. LASSO Feature Selection on Fused Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO for feature selection\n",
    "print(\"Performing LASSO feature selection on fused features...\")\n",
    "print(f\"Training LASSO on {len(all_feature_names)} fused features...\")\n",
    "\n",
    "# Use LassoCV to find optimal alpha\n",
    "lasso_cv = LassoCV(\n",
    "    cv=5,\n",
    "    max_iter=LASSO_MAX_ITER,\n",
    "    alphas=np.logspace(np.log10(LASSO_ALPHA_MIN), np.log10(LASSO_ALPHA_MAX), 100),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training LassoCV (this may take a moment)...\")\n",
    "lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "print(f\"\\nLASSO training completed:\")\n",
    "print(f\"  Optimal alpha: {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"  CV score (RÂ²): {lasso_cv.score(X_scaled, y):.4f}\")\n",
    "\n",
    "# Get selected features\n",
    "selected_features_mask = lasso_cv.coef_ != 0\n",
    "selected_features = np.array(all_feature_names)[selected_features_mask]\n",
    "selected_indices = np.where(selected_features_mask)[0]\n",
    "\n",
    "print(f\"\\nFeature selection results:\")\n",
    "print(f\"  Original fused features: {len(all_feature_names)}\")\n",
    "print(f\"  Selected features: {len(selected_features)}\")\n",
    "print(f\"  Selection ratio: {len(selected_features) / len(all_feature_names) * 100:.1f}%\")\n",
    "\n",
    "# Categorize selected features\n",
    "cnn_selected = sum(1 for f in selected_features if f in cnn_feature_names)\n",
    "radiograph_selected = sum(1 for f in selected_features if f in radiograph_feature_names)\n",
    "\n",
    "print(f\"\\nFeature source breakdown:\")\n",
    "print(f\"  CNN features selected: {cnn_selected}/{len(cnn_feature_names)}\")\n",
    "print(f\"  Radiograph features selected: {radiograph_selected}/{len(radiograph_feature_names)}\")\n",
    "\n",
    "print(f\"\\nTop selected features:\")\n",
    "top_coefs_idx = np.argsort(np.abs(lasso_cv.coef_[selected_features_mask]))[-10:][::-1]\n",
    "for i, idx in enumerate(top_coefs_idx, 1):\n",
    "    feat_idx = selected_indices[idx]\n",
    "    feature_source = 'CNN' if all_feature_names[feat_idx] in cnn_feature_names else 'Radiograph'\n",
    "    print(f\"  {i}. {all_feature_names[feat_idx]:50s} [{feature_source:10s}] (coef: {lasso_cv.coef_[feat_idx]:8.4f})\")\n",
    "\n",
    "# Save LASSO model and selected features\n",
    "lasso_path = OUTPUT_DIR / 'lasso_model.pkl'\n",
    "with open(lasso_path, 'wb') as f:\n",
    "    pickle.dump(lasso_cv, f)\n",
    "\n",
    "selected_features_info = {\n",
    "    'feature_names': selected_features.tolist(),\n",
    "    'feature_indices': selected_indices.tolist(),\n",
    "    'feature_sources': ['CNN' if f in cnn_feature_names else 'Radiograph' for f in selected_features],\n",
    "    'coefficients': lasso_cv.coef_[selected_features_mask].tolist(),\n",
    "    'alpha': lasso_cv.alpha_,\n",
    "    'cv_score': lasso_cv.score(X_scaled, y),\n",
    "    'cnn_selected_count': int(cnn_selected),\n",
    "    'radiograph_selected_count': int(radiograph_selected)\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'selected_features.json', 'w') as f:\n",
    "    json.dump(selected_features_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nLASSO model saved to: {lasso_path}\")\n",
    "print(f\"Selected features info saved to: {OUTPUT_DIR / 'selected_features.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. LASSO Coefficients Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LASSO coefficients\n",
    "print(\"Visualizing LASSO coefficients...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Alpha path\n",
    "axes[0].loglog(lasso_cv.alphas_, lasso_cv.mse_path_.mean(axis=1))\n",
    "axes[0].axvline(lasso_cv.alpha_, color='r', linestyle='--', label=f'Optimal alpha: {lasso_cv.alpha_:.4f}')\n",
    "axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[0].set_ylabel('Mean Squared Error')\n",
    "axes[0].set_title('LASSO: Alpha Selection')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top coefficients\n",
    "top_n = 20\n",
    "top_indices = np.argsort(np.abs(lasso_cv.coef_))[-top_n:][::-1]\n",
    "top_features = [all_feature_names[i] for i in top_indices]\n",
    "top_coefs = lasso_cv.coef_[top_indices]\n",
    "top_sources = ['CNN' if f in cnn_feature_names else 'Radiograph' for f in top_features]\n",
    "\n",
    "colors = ['#1f77b4' if s == 'CNN' else '#ff7f0e' for s in top_sources]\n",
    "axes[1].barh(range(len(top_coefs)), top_coefs, color=colors)\n",
    "axes[1].set_yticks(range(len(top_coefs)))\n",
    "axes[1].set_yticklabels(top_features, fontsize=9)\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "axes[1].set_title(f'Top {top_n} LASSO Coefficients (Blue=CNN, Orange=Radiograph)')\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'lasso_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"LASSO coefficients plot saved to: {OUTPUT_DIR / 'lasso_coefficients.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Classification Model using Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use selected features for classification\n",
    "print(\"Training classification model with selected fused features...\")\n",
    "\n",
    "X_selected = X_scaled[:, selected_features_mask]\n",
    "\n",
    "print(f\"\\nUsing {len(selected_features)} selected features for classification\")\n",
    "print(f\"Feature matrix shape: {X_selected.shape}\")\n",
    "\n",
    "# Train logistic regression classifier\n",
    "classifier = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "classifier.fit(X_selected, y)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Make predictions on training data\n",
    "y_pred = classifier.predict(X_selected)\n",
    "y_pred_proba = classifier.predict_proba(X_selected)\n",
    "\n",
    "print(f\"\\nPredictions generated:\")\n",
    "print(f\"  Predicted labels shape: {y_pred.shape}\")\n",
    "print(f\"  Prediction probabilities shape: {y_pred_proba.shape}\")\n",
    "\n",
    "# Save classifier\n",
    "classifier_path = OUTPUT_DIR / 'classifier_model.pkl'\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "print(f\"\\nClassifier saved to: {classifier_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION PERFORMANCE METRICS (FUSED FEATURES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic metrics\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision_weighted = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nOverall Metrics (weighted average):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "print(f\"  Recall:    {recall_weighted:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_weighted:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for class_label in np.unique(y):\n",
    "    mask = y == class_label\n",
    "    class_acc = accuracy_score(y[mask], y_pred[mask])\n",
    "    class_prec = precision_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n",
    "    class_rec = recall_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n",
    "    class_f1 = f1_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n  Class {class_label}: ({mask.sum()} samples)\")\n",
    "    print(f\"    Accuracy:  {class_acc:.4f}\")\n",
    "    print(f\"    Precision: {class_prec:.4f}\")\n",
    "    print(f\"    Recall:    {class_rec:.4f}\")\n",
    "    print(f\"    F1-Score:  {class_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nConfusion Matrix shape: {cm.shape}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(NUM_CLASSES), \n",
    "            yticklabels=range(NUM_CLASSES),\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax)\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('Confusion Matrix (Fused Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion matrix plot saved to: {OUTPUT_DIR / 'confusion_matrix.png'}\")\n",
    "\n",
    "# Save confusion matrix\n",
    "np.save(OUTPUT_DIR / 'confusion_matrix.npy', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y, y_pred, target_names=[f'Class {i}' for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# Save classification report\n",
    "report_dict = classification_report(y, y_pred, output_dict=True, \n",
    "                                   target_names=[f'Class {i}' for i in range(NUM_CLASSES)])\n",
    "with open(OUTPUT_DIR / 'classification_report.json', 'w') as f:\n",
    "    json.dump(report_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nClassification report saved to: {OUTPUT_DIR / 'classification_report.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. ROC Curves (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for each class\n",
    "print(\"\\nCalculating ROC curves...\")\n",
    "\n",
    "y_bin = label_binarize(y, classes=range(NUM_CLASSES))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "roc_auc_scores = {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_scores[f'Class {i}'] = roc_auc\n",
    "    \n",
    "    axes[i].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    axes[i].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[i].set_xlim([0.0, 1.0])\n",
    "    axes[i].set_ylim([0.0, 1.05])\n",
    "    axes[i].set_xlabel('False Positive Rate')\n",
    "    axes[i].set_ylabel('True Positive Rate')\n",
    "    axes[i].set_title(f'ROC Curve - Class {i}')\n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove the extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Scores:\")\n",
    "for class_name, score in roc_auc_scores.items():\n",
    "    print(f\"  {class_name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nROC curves plot saved to: {OUTPUT_DIR / 'roc_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Metrics Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive metrics dictionary\n",
    "metrics_summary = {\n",
    "    'fusion_info': {\n",
    "        'cnn_features_count': len(cnn_feature_names),\n",
    "        'radiograph_features_count': len(radiograph_feature_names),\n",
    "        'total_fused_features': len(all_feature_names)\n",
    "    },\n",
    "    'overall_metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision_weighted': float(precision_weighted),\n",
    "        'recall_weighted': float(recall_weighted),\n",
    "        'f1_score_weighted': float(f1_weighted)\n",
    "    },\n",
    "    'roc_auc_scores': roc_auc_scores,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'model_info': {\n",
    "        'fused_features_before_selection': len(all_feature_names),\n",
    "        'selected_features': len(selected_features),\n",
    "        'cnn_selected': int(cnn_selected),\n",
    "        'radiograph_selected': int(radiograph_selected),\n",
    "        'selection_ratio': float(len(selected_features) / len(all_feature_names)),\n",
    "        'pca_components': NUM_PCA_COMPONENTS,\n",
    "        'pca_explained_variance': float(np.cumsum(pca.explained_variance_ratio_)[-1]),\n",
    "        'lasso_alpha': float(lasso_cv.alpha_),\n",
    "        'total_samples': len(y),\n",
    "        'num_classes': NUM_CLASSES\n",
    "    },\n",
    "    'classification_report': report_dict\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "with open(OUTPUT_DIR / 'metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METRICS SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nAll metrics saved to: {OUTPUT_DIR / 'metrics_summary.json'}\")\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "for file in sorted(OUTPUT_DIR.glob('*')):\n",
    "    if file.is_file():\n",
    "        print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUSION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. FEATURE FUSION\")\n",
    "print(f\"   - CNN features: {len(cnn_feature_names)}\")\n",
    "print(f\"   - Radiograph features: {len(radiograph_feature_names)}\")\n",
    "print(f\"   - Total fused features: {len(all_feature_names)}\")\n",
    "print(f\"   - Samples with both modalities: {len(merged_df)}\")\n",
    "\n",
    "print(f\"\\n2. PCA ANALYSIS\")\n",
    "print(f\"   - Reduced {len(all_feature_names)} fused features to {NUM_PCA_COMPONENTS} components\")\n",
    "print(f\"   - Retained {np.cumsum(pca.explained_variance_ratio_)[-1]:.2%} of variance\")\n",
    "\n",
    "print(f\"\\n3. LASSO FEATURE SELECTION\")\n",
    "print(f\"   - Selected {len(selected_features)} features ({len(selected_features)/len(all_feature_names)*100:.1f}%)\")\n",
    "print(f\"   - CNN features selected: {cnn_selected}/{len(cnn_feature_names)}\")\n",
    "print(f\"   - Radiograph features selected: {radiograph_selected}/{len(radiograph_feature_names)}\")\n",
    "print(f\"   - Optimal regularization alpha: {lasso_cv.alpha_:.6f}\")\n",
    "\n",
    "print(f\"\\n4. CLASSIFICATION PERFORMANCE\")\n",
    "print(f\"   - Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   - Weighted Precision: {precision_weighted:.4f}\")\n",
    "print(f\"   - Weighted Recall: {recall_weighted:.4f}\")\n",
    "print(f\"   - Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\n5. CLASS-SPECIFIC PERFORMANCE\")\n",
    "for class_label in np.unique(y):\n",
    "    mask = y == class_label\n",
    "    class_acc = accuracy_score(y[mask], y_pred[mask])\n",
    "    print(f\"   - Class {class_label}: {class_acc:.4f} accuracy ({mask.sum()} samples)\")\n",
    "\n",
    "print(f\"\\n6. OUTPUT DIRECTORY\")\n",
    "print(f\"   - Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"   - Plots: PCA variance, scatter plots, LASSO coefficients, confusion matrix, ROC curves\")\n",
    "print(f\"   - Models: PCA, LASSO, Classifier, Feature scaler\")\n",
    "print(f\"   - Metrics: Comprehensive JSON summary with fusion details\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
