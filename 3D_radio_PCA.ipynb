{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Radiomic Features - PCA and LASSO Classification\n",
    "## Dimensionality Reduction and Feature Selection for Classification\n",
    "\n",
    "This notebook:\n",
    "- Loads radiomic features and sample classifications\n",
    "- Performs PCA for dimensionality reduction and visualization\n",
    "- Uses LASSO regression for feature selection\n",
    "- Trains and evaluates classification model\n",
    "- Reports comprehensive ML metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path('C:/FeatureEx')\n",
    "RADIOMICS_DIR = BASE_DIR / 'radiomics_3d'\n",
    "METADATA_FILE = BASE_DIR / 'classification_metadata.xlsx'\n",
    "IMAGES_DIR = BASE_DIR / 'preprocessed_3d_data' / 'images'\n",
    "OUTPUT_DIR = BASE_DIR / 'radio_pca_results'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "NUM_PCA_COMPONENTS = 50  # Number of PCA components to retain\n",
    "NUM_CLASSES = 5  # Number of classification classes\n",
    "CLASS_NAMES = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "LASSO_MAX_ITER = 5000\n",
    "LASSO_ALPHA_MIN = 0.0001\n",
    "LASSO_ALPHA_MAX = 1.0\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Radiomics directory: {RADIOMICS_DIR}\")\n",
    "print(f\"Metadata file: {METADATA_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  PCA components: {NUM_PCA_COMPONENTS}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"  LASSO alpha range: [{LASSO_ALPHA_MIN}, {LASSO_ALPHA_MAX}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Radiomic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load radiomic features\n",
    "print(\"Loading radiomic features...\")\nfeatures_pkl = RADIOMICS_DIR / 'radiomics_3d_features.pkl'\nwith open(features_pkl, 'rb') as f:\n    features_data = pickle.load(f)\n\nfeatures_df = features_data['features_df']\nfeature_names = features_data['feature_names']\nsample_ids = features_data['sample_ids']\nmetadata = features_data['metadata']\n\nprint(f\"Loaded features:\")\nprint(f\"  Samples: {len(features_df)}\")\nprint(f\"  Features: {len(feature_names)}\")\nprint(f\"  Feature names sample: {feature_names[:5]}\")\n\n# Verify sample_id column\nif 'sample_id' not in features_df.columns:\n    features_df['sample_id'] = sample_ids\n    print(f\"  Added sample_id column\")\n\nprint(f\"\\nDataFrame shape: {features_df.shape}\")\nprint(f\"DataFrame columns (first 10): {list(features_df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Classification Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading classification metadata...\")\nmetadata_df = pd.read_excel(METADATA_FILE, sheet_name='samples')\n\nprint(f\"Loaded metadata:\")\nprint(f\"  Samples: {len(metadata_df)}\")\nprint(f\"  Columns: {list(metadata_df.columns)}\")\nprint(f\"\\nMetadata preview:\")\nprint(metadata_df.head())\n\nprint(f\"\\nClass distribution:\")\nprint(metadata_df['label'].value_counts().sort_index())\n\nprint(f\"\\nData split:\")\nprint(metadata_df['split'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Features with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features with labels\nprint(\"Merging features with classification labels...\")\n\n# Create mapping from sample_id to label\nlabel_map = dict(zip(metadata_df['sample_id'], metadata_df['label']))\n\n# Add labels to features dataframe\nfeatures_df['class_label'] = features_df['sample_id'].map(label_map)\n\n# Check for unmatched samples\nunmatched = features_df['class_label'].isna().sum()\nif unmatched > 0:\n    print(f\"WARNING: {unmatched} samples have no matching label\")\n    print(f\"Removing unmatched samples...\")\n    features_df = features_df.dropna(subset=['class_label'])\n    features_df['class_label'] = features_df['class_label'].astype(int)\n\nprint(f\"\\nMerged data:\")\nprint(f\"  Total samples with labels: {len(features_df)}\")\nprint(f\"  Total features: {len(feature_names)}\")\nprint(f\"\\nClass distribution in merged data:\")\nprint(features_df['class_label'].value_counts().sort_index())\n\n# Prepare data for analysis\nX = features_df[feature_names].values  # Feature matrix\ny = features_df['class_label'].values   # Labels\nsample_ids_final = features_df['sample_id'].values\n\nprint(f\"\\nData shapes:\")\nprint(f\"  X (features): {X.shape}\")\nprint(f\"  y (labels): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\nprint(\"Standardizing features...\")\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"Feature scaling completed:\")\nprint(f\"  Mean (should be ~0): {X_scaled.mean(axis=0)[:5]}\")\nprint(f\"  Std (should be ~1): {X_scaled.std(axis=0)[:5]}\")\n\n# Save scaler for later use\nscaler_path = OUTPUT_DIR / 'feature_scaler.pkl'\nwith open(scaler_path, 'wb') as f:\n    pickle.dump(scaler, f)\nprint(f\"\\nScaler saved to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\nprint(\"Performing PCA...\")\npca = PCA(n_components=NUM_PCA_COMPONENTS)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"PCA completed:\")\nprint(f\"  Components: {pca.n_components_}\")\nprint(f\"  Explained variance ratio: {pca.explained_variance_ratio_[:5]}\")\nprint(f\"  Cumulative variance: {np.cumsum(pca.explained_variance_ratio_)[-1]:.4f}\")\n\n# Plot explained variance\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scree plot\naxes[0].bar(range(1, NUM_PCA_COMPONENTS + 1), pca.explained_variance_ratio_)\naxes[0].set_xlabel('Principal Component')\naxes[0].set_ylabel('Explained Variance Ratio')\naxes[0].set_title('PCA Scree Plot')\naxes[0].grid(True, alpha=0.3)\n\n# Cumulative explained variance\naxes[1].plot(np.cumsum(pca.explained_variance_ratio_))\naxes[1].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\naxes[1].set_xlabel('Number of Components')\naxes[1].set_ylabel('Cumulative Explained Variance')\naxes[1].set_title('Cumulative Explained Variance')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'pca_variance_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nPCA plot saved to: {OUTPUT_DIR / 'pca_variance_analysis.png'}\")\n\n# Save PCA model\npca_path = OUTPUT_DIR / 'pca_model.pkl'\nwith open(pca_path, 'wb') as f:\n    pickle.dump(pca, f)\nprint(f\"PCA model saved to: {pca_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PCA Visualization by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\nprint(\"Visualizing PCA results...\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 2D PCA plot (PC1 vs PC2)\nfor class_label in np.unique(y):\n    mask = y == class_label\n    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n                   label=f'Class {class_label}', alpha=0.7, s=50)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} var)')\naxes[0].set_title('PCA: PC1 vs PC2')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 2D PCA plot (PC1 vs PC3)\nfor class_label in np.unique(y):\n    mask = y == class_label\n    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 2], \n                   label=f'Class {class_label}', alpha=0.7, s=50)\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)')\naxes[1].set_ylabel(f'PC3 ({pca.explained_variance_ratio_[2]:.2%} var)')\naxes[1].set_title('PCA: PC1 vs PC3')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'pca_scatter_plot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"PCA scatter plot saved to: {OUTPUT_DIR / 'pca_scatter_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LASSO Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO for feature selection\nprint(\"Performing LASSO feature selection...\")\nprint(f\"Training LASSO on original {len(feature_names)} features...\")\n\n# Use LassoCV to find optimal alpha\nlasso_cv = LassoCV(\n    cv=5,\n    max_iter=LASSO_MAX_ITER,\n    alphas=np.logspace(np.log10(LASSO_ALPHA_MIN), np.log10(LASSO_ALPHA_MAX), 100),\n    random_state=42\n)\n\nprint(\"Training LassoCV (this may take a moment)...\")\nlasso_cv.fit(X_scaled, y)\n\nprint(f\"\\nLASSO training completed:\")\nprint(f\"  Optimal alpha: {lasso_cv.alpha_:.6f}\")\nprint(f\"  CV score (RÂ²): {lasso_cv.score(X_scaled, y):.4f}\")\n\n# Get selected features\nselected_features_mask = lasso_cv.coef_ != 0\nselected_features = np.array(feature_names)[selected_features_mask]\nselected_indices = np.where(selected_features_mask)[0]\n\nprint(f\"\\nFeature selection results:\")\nprint(f\"  Original features: {len(feature_names)}\")\nprint(f\"  Selected features: {len(selected_features)}\")\nprint(f\"  Selection ratio: {len(selected_features) / len(feature_names) * 100:.1f}%\")\n\nprint(f\"\\nTop selected features:\")\ntop_coefs_idx = np.argsort(np.abs(lasso_cv.coef_[selected_features_mask]))[-10:][::-1]\nfor i, idx in enumerate(top_coefs_idx, 1):\n    feat_idx = selected_indices[idx]\n    print(f\"  {i}. {feature_names[feat_idx]:50s} (coef: {lasso_cv.coef_[feat_idx]:8.4f})\")\n\n# Save LASSO model and selected features\nlasso_path = OUTPUT_DIR / 'lasso_model.pkl'\nwith open(lasso_path, 'wb') as f:\n    pickle.dump(lasso_cv, f)\n\nselected_features_info = {\n    'feature_names': selected_features.tolist(),\n    'feature_indices': selected_indices.tolist(),\n    'coefficients': lasso_cv.coef_[selected_features_mask].tolist(),\n    'alpha': lasso_cv.alpha_,\n    'cv_score': lasso_cv.score(X_scaled, y)\n}\n\nwith open(OUTPUT_DIR / 'selected_features.json', 'w') as f:\n    json.dump(selected_features_info, f, indent=2)\n\nprint(f\"\\nLASSO model saved to: {lasso_path}\")\nprint(f\"Selected features info saved to: {OUTPUT_DIR / 'selected_features.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LASSO Coefficients Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LASSO coefficients\nprint(\"Visualizing LASSO coefficients...\")\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n# Alpha path\naxes[0].loglog(lasso_cv.alphas_, lasso_cv.mse_path_.mean(axis=1))\naxes[0].axvline(lasso_cv.alpha_, color='r', linestyle='--', label=f'Optimal alpha: {lasso_cv.alpha_:.4f}')\naxes[0].set_xlabel('Alpha (Regularization Strength)')\naxes[0].set_ylabel('Mean Squared Error')\naxes[0].set_title('LASSO: Alpha Selection')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Top coefficients\ntop_n = 20\ntop_indices = np.argsort(np.abs(lasso_cv.coef_))[-top_n:][::-1]\ntop_features = [feature_names[i] for i in top_indices]\ntop_coefs = lasso_cv.coef_[top_indices]\n\ncolors = ['green' if c > 0 else 'red' for c in top_coefs]\naxes[1].barh(range(len(top_coefs)), top_coefs, color=colors)\naxes[1].set_yticks(range(len(top_coefs)))\naxes[1].set_yticklabels(top_features, fontsize=9)\naxes[1].set_xlabel('Coefficient Value')\naxes[1].set_title(f'Top {top_n} LASSO Coefficients')\naxes[1].axvline(0, color='black', linestyle='-', linewidth=0.5)\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'lasso_coefficients.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"LASSO coefficients plot saved to: {OUTPUT_DIR / 'lasso_coefficients.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train Classification Model using Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use selected features for classification\nprint(\"Training classification model with selected features...\")\n\nX_selected = X_scaled[:, selected_features_mask]\n\nprint(f\"\\nUsing {len(selected_features)} selected features for classification\")\nprint(f\"Feature matrix shape: {X_selected.shape}\")\n\n# Train logistic regression classifier\nclassifier = OneVsRestClassifier(\n    LogisticRegression(max_iter=1000, random_state=42)\n)\n\nprint(\"Training classifier...\")\nclassifier.fit(X_selected, y)\nprint(\"Training completed!\")\n\n# Make predictions on training data\ny_pred = classifier.predict(X_selected)\ny_pred_proba = classifier.predict_proba(X_selected)\n\nprint(f\"\\nPredictions generated:\")\nprint(f\"  Predicted labels shape: {y_pred.shape}\")\nprint(f\"  Prediction probabilities shape: {y_pred_proba.shape}\")\n\n# Save classifier\nclassifier_path = OUTPUT_DIR / 'classifier_model.pkl'\nwith open(classifier_path, 'wb') as f:\n    pickle.dump(classifier, f)\nprint(f\"\\nClassifier saved to: {classifier_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\nprint(\"\\n\" + \"=\"*70)\nprint(\"CLASSIFICATION PERFORMANCE METRICS\")\nprint(\"=\"*70)\n\n# Basic metrics\naccuracy = accuracy_score(y, y_pred)\nprecision_weighted = precision_score(y, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y, y_pred, average='weighted', zero_division=0)\nf1_weighted = f1_score(y, y_pred, average='weighted', zero_division=0)\n\nprint(f\"\\nOverall Metrics (weighted average):\")\nprint(f\"  Accuracy:  {accuracy:.4f}\")\nprint(f\"  Precision: {precision_weighted:.4f}\")\nprint(f\"  Recall:    {recall_weighted:.4f}\")\nprint(f\"  F1-Score:  {f1_weighted:.4f}\")\n\n# Per-class metrics\nprint(f\"\\nPer-Class Metrics:\")\nfor class_label in np.unique(y):\n    mask = y == class_label\n    class_acc = accuracy_score(y[mask], y_pred[mask])\n    class_prec = precision_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n    class_rec = recall_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n    class_f1 = f1_score(y[mask], y_pred[mask], average='weighted', zero_division=0)\n    \n    print(f\"\\n  Class {class_label}: ({mask.sum()} samples)\")\n    print(f\"    Accuracy:  {class_acc:.4f}\")\n    print(f\"    Precision: {class_prec:.4f}\")\n    print(f\"    Recall:    {class_rec:.4f}\")\n    print(f\"    F1-Score:  {class_f1:.4f}\")\n\nprint(f\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nprint(f\"\\nConfusion Matrix shape: {cm.shape}\")\n\n# Visualize confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=range(NUM_CLASSES), \n            yticklabels=range(NUM_CLASSES),\n            cbar_kws={'label': 'Count'},\n            ax=ax)\nax.set_xlabel('Predicted Label')\nax.set_ylabel('True Label')\nax.set_title('Confusion Matrix')\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nConfusion matrix plot saved to: {OUTPUT_DIR / 'confusion_matrix.png'}\")\n\n# Save confusion matrix\nnp.save(OUTPUT_DIR / 'confusion_matrix.npy', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\nprint(\"\\n\" + \"=\"*70)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\"*70)\nprint(classification_report(y, y_pred, target_names=[f'Class {i}' for i in range(NUM_CLASSES)]))\n\n# Save classification report\nreport_dict = classification_report(y, y_pred, output_dict=True, \n                                   target_names=[f'Class {i}' for i in range(NUM_CLASSES)])\nwith open(OUTPUT_DIR / 'classification_report.json', 'w') as f:\n    json.dump(report_dict, f, indent=2)\n\nprint(f\"\\nClassification report saved to: {OUTPUT_DIR / 'classification_report.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ROC Curves (One-vs-Rest)"
   ]
  },
  {
   "cell_type": {"markdown"}, "source": [
    "## 15. ROC Curves (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for each class\nprint(\"\\nCalculating ROC curves...\")\n\ny_bin = label_binarize(y, classes=range(NUM_CLASSES))\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nroc_auc_scores = {}\n\nfor i in range(NUM_CLASSES):\n    fpr, tpr, _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])\n    roc_auc = auc(fpr, tpr)\n    roc_auc_scores[f'Class {i}'] = roc_auc\n    \n    axes[i].plot(fpr, tpr, color='darkorange', lw=2, \n                label=f'ROC curve (AUC = {roc_auc:.3f})')\n    axes[i].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    axes[i].set_xlim([0.0, 1.0])\n    axes[i].set_ylim([0.0, 1.05])\n    axes[i].set_xlabel('False Positive Rate')\n    axes[i].set_ylabel('True Positive Rate')\n    axes[i].set_title(f'ROC Curve - Class {i}')\n    axes[i].legend(loc=\"lower right\")\n    axes[i].grid(True, alpha=0.3)\n\n# Remove the extra subplot\nfig.delaxes(axes[5])\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'roc_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nROC-AUC Scores:\")\nfor class_name, score in roc_auc_scores.items():\n    print(f\"  {class_name}: {score:.4f}\")\n\nprint(f\"\\nROC curves plot saved to: {OUTPUT_DIR / 'roc_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Metrics Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive metrics dictionary\nmetrics_summary = {\n",
    "    'overall_metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision_weighted': float(precision_weighted),\n",
    "        'recall_weighted': float(recall_weighted),\n",
    "        'f1_score_weighted': float(f1_weighted)\n",
    "    },\n",
    "    'roc_auc_scores': roc_auc_scores,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'model_info': {\n",
    "        'original_features': len(feature_names),\n",
    "        'selected_features': len(selected_features),\n",
    "        'selection_ratio': float(len(selected_features) / len(feature_names)),\n",
    "        'pca_components': NUM_PCA_COMPONENTS,\n",
    "        'pca_explained_variance': float(np.cumsum(pca.explained_variance_ratio_)[-1]),\n",
    "        'lasso_alpha': float(lasso_cv.alpha_),\n",
    "        'total_samples': len(y),\n",
    "        'num_classes': NUM_CLASSES\n",
    "    },\n",
    "    'classification_report': report_dict\n",
    "}\n",
    "\n",
    "# Save metrics\nwith open(OUTPUT_DIR / 'metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"METRICS SAVED\")\nprint(\"=\"*70)\nprint(f\"\\nAll metrics saved to: {OUTPUT_DIR / 'metrics_summary.json'}\")\n\nprint(f\"\\nGenerated files:\")\nfor file in sorted(OUTPUT_DIR.glob('*')):\n    if file.is_file():\n        print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\nprint(\"ANALYSIS SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\n1. PCA ANALYSIS\")\nprint(f\"   - Reduced {len(feature_names)} features to {NUM_PCA_COMPONENTS} components\")\nprint(f\"   - Retained {np.cumsum(pca.explained_variance_ratio_)[-1]:.2%} of variance\")\n\nprint(f\"\\n2. LASSO FEATURE SELECTION\")\nprint(f\"   - Selected {len(selected_features)} features ({len(selected_features)/len(feature_names)*100:.1f}%)\")\nprint(f\"   - Optimal regularization alpha: {lasso_cv.alpha_:.6f}\")\n\nprint(f\"\\n3. CLASSIFICATION PERFORMANCE\")\nprint(f\"   - Overall Accuracy: {accuracy:.4f}\")\nprint(f\"   - Weighted Precision: {precision_weighted:.4f}\")\nprint(f\"   - Weighted Recall: {recall_weighted:.4f}\")\nprint(f\"   - Weighted F1-Score: {f1_weighted:.4f}\")\n\nprint(f\"\\n4. CLASS-SPECIFIC PERFORMANCE\")\nfor class_label in np.unique(y):\n    mask = y == class_label\n    class_acc = accuracy_score(y[mask], y_pred[mask])\n    print(f\"   - Class {class_label}: {class_acc:.4f} accuracy ({mask.sum()} samples)\")\n\nprint(f\"\\n5. OUTPUT DIRECTORY\")\nprint(f\"   - Results saved to: {OUTPUT_DIR}\")\nprint(f\"   - Plots: PCA, LASSO coefficients, confusion matrix, ROC curves\")\nprint(f\"   - Models: PCA, LASSO, Classifier\")\nprint(f\"   - Metrics: Comprehensive JSON summary\")\n\nprint(f\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-text",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
